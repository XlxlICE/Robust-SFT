{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shubing/opt/anaconda3/envs/SFT/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/shubing/opt/anaconda3/envs/SFT/lib/python3.13/site-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "/Users/shubing/opt/anaconda3/envs/SFT/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `multi_modality` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/SFT/lib/python3.13/site-packages/transformers/models/auto/configuration_auto.py:1131\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1130\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1131\u001b[39m     config_class = \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/SFT/lib/python3.13/site-packages/transformers/models/auto/configuration_auto.py:833\u001b[39m, in \u001b[36m_LazyConfigMapping.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mapping:\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    834\u001b[39m value = \u001b[38;5;28mself\u001b[39m._mapping[key]\n",
      "\u001b[31mKeyError\u001b[39m: 'multi_modality'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Load tokenizer and model\u001b[39;00m\n\u001b[32m     25\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Apply LoRA\u001b[39;00m\n\u001b[32m     29\u001b[39m lora_config = LoraConfig(\n\u001b[32m     30\u001b[39m     r=\u001b[32m8\u001b[39m,\n\u001b[32m     31\u001b[39m     lora_alpha=\u001b[32m16\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     task_type=\u001b[33m\"\u001b[39m\u001b[33mCAUSAL_LM\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     36\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/SFT/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:531\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    529\u001b[39m     _ = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m config, kwargs = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[32m    542\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig.get(\u001b[33m\"\u001b[39m\u001b[33mtorch_dtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/SFT/lib/python3.13/site-packages/transformers/models/auto/configuration_auto.py:1133\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1131\u001b[39m         config_class = CONFIG_MAPPING[config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m   1132\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1133\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1134\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[33m'\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1135\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1136\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1137\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou can update Transformers with the command `pip install --upgrade transformers`. If this \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1138\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mdoes not work, and the checkpoint is very new, then there may not be a release version \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1139\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mthat supports this model yet. In this case, you can get the most up-to-date code by installing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1140\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTransformers from source with the command \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1141\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`pip install git+https://github.com/huggingface/transformers.git`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1142\u001b[39m         )\n\u001b[32m   1143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class.from_dict(config_dict, **unused_kwargs)\n\u001b[32m   1144\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1145\u001b[39m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[32m   1146\u001b[39m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: The checkpoint you are trying to load has model type `multi_modality` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login\n",
    "from peft import LoraConfig, get_peft_model  # <<< 加入LoRA\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Set model and dataset paths\n",
    "model_name = \"deepseek-ai/deepseek-vl-1.3b-chat\"\n",
    "data_path = \"mixed_sft/sft_rho5.jsonl\"\n",
    "output_dir = \"sft_models/sft_rho5\"\n",
    "token = \"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\"\n",
    "\n",
    "# Login to Hugging Face\n",
    "login(token=token)\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=token)\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # 注意需要确认模型内部名字，暂时用常规名字\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "def load_local_dataset(path, sample_size=None, seed=42):\n",
    "    \"\"\"\n",
    "    Load a local JSONL dataset and optionally sample a fixed number of entries.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the local JSONL file.\n",
    "        sample_size (int, optional): Number of samples to randomly select.\n",
    "                                     If None, load the full dataset.\n",
    "        seed (int): Random seed to ensure reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: A HuggingFace Dataset object containing the loaded or sampled data.\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    \n",
    "    if sample_size is not None and sample_size < len(data):\n",
    "        random.seed(seed)\n",
    "        data = random.sample(data, sample_size)\n",
    "\n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "# Load and optionally sample training data\n",
    "dataset = load_local_dataset(data_path, sample_size=None)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize(example):\n",
    "    prompt = example[\"prompt\"]\n",
    "    answer = example[\"answer\"]\n",
    "    text = prompt + \"\\n\\n\" + answer\n",
    "    return tokenizer(text, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    logging_steps=10,\n",
    "    save_steps=100\n",
    ")\n",
    "\n",
    "# Data collator for causal language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'prepare_model_for_int8_training' from 'peft' (/Users/shubing/opt/anaconda3/envs/SFT/lib/python3.13/site-packages/peft/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfinetuning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m main\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 定义你的数据集对应的超参数\u001b[39;00m\n\u001b[32m      4\u001b[39m dataset_paths = {\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrho0\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmixed_sft/sft_rho0.jsonl\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrho5\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmixed_sft/sft_rho5.jsonl\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrho10\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmixed_sft/sft_rho10.jsonl\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrho20\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmixed_sft/sft_rho20.jsonl\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Robust-SFT/finetuning.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdist\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_peft_model, prepare_model_for_int8_training\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpkg_resources\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m packaging\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfsdp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     12\u001b[39m     FullyShardedDataParallel \u001b[38;5;28;01mas\u001b[39;00m FSDP,\n\u001b[32m     13\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'prepare_model_for_int8_training' from 'peft' (/Users/shubing/opt/anaconda3/envs/SFT/lib/python3.13/site-packages/peft/__init__.py)"
     ]
    }
   ],
   "source": [
    "from finetuning import main\n",
    "\n",
    "# 定义你的数据集对应的超参数\n",
    "dataset_paths = {\n",
    "    \"rho0\": \"mixed_sft/sft_rho0.jsonl\",\n",
    "    \"rho5\": \"mixed_sft/sft_rho5.jsonl\",\n",
    "    \"rho10\": \"mixed_sft/sft_rho10.jsonl\",\n",
    "    \"rho20\": \"mixed_sft/sft_rho20.jsonl\",\n",
    "}\n",
    "\n",
    "# 遍历不同污染比例\n",
    "for rho_name, data_path in dataset_paths.items():\n",
    "    output_dir = f\"sft_models/{rho_name}_lora\"\n",
    "\n",
    "    # 传入finetuning的参数\n",
    "    args = {\n",
    "        \"model_name\": \"deepseek-ai/deepseek-vl-1.3b-chat\",\n",
    "        \"dataset_path\": data_path,        # 你需要在 train_config 里读取这个\n",
    "        \"output_dir\": output_dir,\n",
    "        \"batch_size_training\": 4,\n",
    "        \"num_epochs\": 3,\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"use_peft\": True,                  # 开启LoRA\n",
    "        \"r\": 8,                            # LoRA rank\n",
    "        \"lora_alpha\": 16,\n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
    "\n",
    "        \"enable_fsdp\": False,               # 暂时单卡就可以，后面你可以切成 True\n",
    "        \"pure_bf16\": False,                 # 用fp16\n",
    "        \"run_validation\": False,\n",
    "        \"save_every_epoch\": True,\n",
    "    }\n",
    "\n",
    "    print(f\"Starting training for {rho_name}\")\n",
    "    main(**args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SFT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
